{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\huynn5\\tensorflow_datasets\\movielens\\100k-ratings\\0.1.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ url]\n",
      "Extraction completed...: 100%|██████████| 23/23 [00:12<00:00,  1.86 file/s]\n",
      "Dl Size...: 100%|██████████| 4/4 [00:12<00:00,  3.10s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:12<00:00, 12.40s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to C:\\Users\\huynn5\\tensorflow_datasets\\movielens\\100k-ratings\\0.1.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\huynn5\\tensorflow_datasets\\movielens\\100k-movies\\0.1.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 142.88 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 100.01 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 76.93 url/s] \n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Dl Size...: 100%|██████████| 4924029/4924029 [00:00<00:00, 289637260.97 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 50.00 url/s]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to C:\\Users\\huynn5\\tensorflow_datasets\\movielens\\100k-movies\\0.1.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Ratings data\n",
    "ratings = tfds.load('movielens/100k-ratings', split='train')\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load('movielens/100k-movies', split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7], dtype=int64),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4], dtype=int64),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed = 42, reshuffle_each_iteration = False)\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.take(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m movie_titles \u001b[38;5;241m=\u001b[39m movies\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1_000\u001b[39m)\n\u001b[0;32m      7\u001b[0m user_ids \u001b[38;5;241m=\u001b[39m ratings\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1_000_000\u001b[39m)\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m unique_movie_titles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmovie_titles\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m      9\u001b[0m unique_user_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[38;5;28mlist\u001b[39m(user_ids)))\n\u001b[0;32m     10\u001b[0m unique_movie_titles[:\u001b[38;5;241m10\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\huynn5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huynn5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huynn5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huynn5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# Figure out unique user ids and movie titles present in the data\n",
    "# Map the raw values of our categorical features to embedding vectors in our model\n",
    "# Need a vocabulary that maps a raw feature value to an integer in a continguous range:\n",
    "# Allow us to look up the corresponding embeddings in our embedding tables\n",
    "\n",
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x['user_id'])\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "unique_movie_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
